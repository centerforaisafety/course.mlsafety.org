---
layout: page
title: Readings
description: TODO The weekly event schedule.
---

# Readings
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

## Introduction

- **[Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)**

- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)

- [Workshop On Safety And Control For Artificial Intelligence](https://www.cmu.edu/safartint/watch.html)

### Deep Learning Review

- **[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)**

- **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)**

- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

- [Layer Normalization](https://arxiv.org/abs/1607.06450)

- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)

- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)

- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

## Risk Analysis

- **[Power laws, Pareto distributions and Zipf’s law](http://tuvalu.santafe.edu/~aaronc/courses/5352/readings/Newman_05_PowerLawsParetoDistributionsAndZipfsLaw.pdf)**

- **[What is a Complex System?](https://www.youtube.com/watch?v=vp8v2Udd_PM)**

- **[Emergence (Intermediate)](https://www.youtube.com/watch?v=QItTWZc7hKs)**

- **Introduction to STAMP ([video](https://www.youtube.com/watch?v=_ptmjAbacMk), [slides](http://psas.scripts.mit.edu/home/wp-content/uploads/2020/07/STAMP-Tutorial.pdf))**

- [Designing for safety, Consequence assessment, Safety models and accident models](https://risk-engineering.org/safe-design/)
- [https://risk-engineering.org/consequence-assessment/](https://risk-engineering.org/safety-models/)

- [A Brief History of Generative Models for Power Law and Lognormal Distributions](https://www.stat.berkeley.edu/~aldous/Networks/1089229510.pdf)

- [Empirical examples of power-law CDFs](https://youtu.be/KKYhPPf4FxA?t=345)

- [Log-normal distributions (with comparisons to power laws)](https://youtu.be/KiTAyRZORtQ?t=124)

- [Multiplicative processes produce log normals](https://www.youtube.com/watch?v=yA1pkyanbzw)

- [Multiplicative processes produce power laws](https://youtu.be/B43yhWxdi5I?t=69)

- [Review of properties of power laws](https://www.youtube.com/watch?v=9Hc227Qy91k)

- [Numerous power laws for cities](https://www.youtube.com/watch?v=DsL7jEQXh8I)

- [The Black Swan](https://www.youtube.com/watch?v=caPy0OZmXKs) and
  [Antifragile](https://www.youtube.com/watch?v=-MMLea-_ifw) Summaries

- [The Precautionary Principle (skip second half)](https://arxiv.org/abs/1410.5787)

- [Emergence (Basic)](https://www.youtube.com/watch?v=16W7c0mb-rE)

- [Nonlinear Causality](https://www.youtube.com/watch?v=76JRJ90s548)

- [Beyond Normal Accidents and High Reliability Organizations: The Need for an Alternative Approach to Safety in Complex Systems](http://sunnyday.mit.edu/papers/hro.pdf)

- [Shortcomings of the Bow Tie and Other Safety Tools Based on Linear Causality1](http://sunnyday.mit.edu/Bow-tie-final.pdf)

- [Systemantics Appendix](https://drive.google.com/file/d/1avoVTY8L3hpZi9fTxI_1mjjXC5JTz882/view?usp=sharing)

- [How Complex Systems Fail](https://how.complexsystems.fail/)


## Adversarial Robustness

- **[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)**

- **[Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)**

- [Data Augmentation Can Improve Robustness](https://arxiv.org/abs/2111.05328)

- [Gradient-based Adversarial Attacks against Text
  Transformers](https://arxiv.org/abs/2104.13733)
  ([GitHub](https://github.com/chbrian/awesome-adversarial-examples-dl))

- [Smooth Adversarial Training](https://arxiv.org/abs/2006.14536)

- [Reliable evaluation of adversarial robustness with an ensemble of diverse
  parameter-free attacks](https://arxiv.org/abs/2003.01690)
  ([website](https://robustbench.github.io/))

- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)

- [Adversarial Examples Are a Natural Consequence of Test Error in Noise](https://arxiv.org/abs/1901.10513)

- [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)

- [Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)

- [Certified Defenses against Adversarial Examples](https://arxiv.org/abs/1801.09344)

- [Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)

- [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)

## Long Tails and Distribution Shift

- **[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/abs/2006.16241)**

- **[Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/abs/1903.12261)**

- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)

- [WILDS: A Benchmark of in-the-Wild Distribution Shifts](https://arxiv.org/abs/2012.07421)

- [ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models](https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html)

- [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/abs/1910.14599)

- [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)

- [ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231)

## Monitoring

### OOD and Malicious Behavior Detection

- **[Deep Anomaly Detection with Outlier Exposure](https://arxiv.org/abs/1812.04606)**

- **[A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://arxiv.org/abs/1610.02136)**

- [ViM: Out-Of-Distribution with Virtual-logit Matching](https://arxiv.org/abs/2203.10807)

- [VOS: Learning What You Don't Know by Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)

- [Scaling Out-of-Distribution Detection for Real-World Settings](https://arxiv.org/abs/1911.11132)

- [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/abs/1807.03888)

### Calibration and Representative Uncertainty

- **[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)**

- **[Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530)**

- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)

- [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474)

- [Posterior calibration and exploratory analysis for natural language processing models](https://arxiv.org/abs/1508.05154)

### Trojans

- **[Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)**

- **[Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs](https://arxiv.org/abs/1906.10842)**

- **[Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)**

- [TrojAI](https://pages.nist.gov/trojai/)

- [Detecting AI Trojans Using Meta Neural Analysis](https://arxiv.org/abs/1910.03137)

- [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)

- [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)

- [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)

### Detecting and Forecasting Emergent Behavior

- **[The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://arxiv.org/abs/2201.03544)**

- **[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)**

- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

- [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683)

- [The Off-Switch Game](https://arxiv.org/abs/1611.08219)

## Alignment

### Power-Seeking

<!--<upcoming power-averseness paper>-->

- [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683)

- [Is power-seeking AI an existential risk?](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#)

### Honest AI

<!--<upcoming collin paper>-->

- [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)

- [Truthful AI: Developing and governing AI that does not lie](https://arxiv.org/abs/2110.06674)

### Machine Ethics

<!--<upcoming utility paper>-->

- **[What Would Jiminy Cricket Do? Towards Agents That Behave Morally](https://arxiv.org/abs/2110.13136)**

- **[Ethics Background (Introduction through "Absolute Rights or Prima Facie Duties")](https://www.youtube.com/playlist?list=PLKtXFotbf7fOg7zbQ3565EnpzzKlYaVVI)**

- [Aligning AI With Shared Human Values](https://arxiv.org/abs/2008.02275)

- [Avoiding Side Effects in Complex Environments](https://arxiv.org/abs/2006.06547)

- [Conservative Agency via Attainable Utility Preservation](https://arxiv.org/abs/1902.09725)

- [The Structure of Normative Ethics ](https://cpb-us-west-2-juc1ugur1qwqqqo4.stackpathdns.com/campuspress.yale.edu/dist/7/724/files/2016/01/The-Structure-of-Normative-Ethics-2gw2akt.pdf)

## Systemic Safety

### Forecasting
<!--<upcoming paper>-->

- [On the Difference between Binary Prediction and True Exposure With Implications For Forecasting Tournaments and Decision Making Research](https://www.stat.berkeley.edu/~aldous/157/Papers/taleb_tetlock.pdf)

- [Superforecasting -- Philip Tetlock](https://youtu.be/pedNak4S9IE?t=440)

### ML for Cyberdefense

#### Intrusion Detection

- [Using Decision Trees to Improve Signature Based Intrusion Detection](https://link.springer.com/chapter/10.1007/978-3-540-45248-5_10)

- [Outside the Closed World: On Using Machine Learning for Network Intrusion Detection](https://ieeexplore.ieee.org/document/5504793)

- [Zero-day malware detection using transferred generative adversarial networks based on deep autoencoders](https://www.sciencedirect.com/science/article/abs/pii/S0020025518303475)

#### Code and software vulnerability analysis

- [Neuzz: Efficient Fuzzing with neural program smoothing](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835342)

- [NeuFuzz: Efficient Fuzzing with deep neural networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672949)

- [Neutaint: Efficient Dynamic Taint Analysis with Neural Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152790)

#### Malware Detection

- [Recognizing Functions in Binaries with Neural Networks](https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-shin.pdf)

- [Analysis of Machine learning Techniques Used in Behavior-Based Malware Detection](https://ieeexplore.ieee.org/abstract/document/5675808)

#### Modeling and prediction of cyber attack response

- [Modeling future cyber attack steps](https://link.springer.com/chapter/10.1007/978-981-15-4474-3_5)

- [Modeling response](https://ieeexplore.ieee.org/abstract/document/9141989)

- [A deep learning framework for predicting cyber attack rates](https://link.springer.com/article/10.1186/s13635-019-0090-6)

#### Code Translation and Code Generation
- [Program Synthesis with Large Language Models](https://arxiv.org/pdf/2108.07732.pdf)

- [Unsupervised Translation of Programming Language](https://arxiv.org/pdf/2006.03511.pdf?id=mDo0CwAAQBAJ&pg=PA347&f=falsesequence=1&v1a=m1xx10&isAllowed=y)

- [Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://arxiv.org/pdf/2108.09293.pdf?nylayout=pc)

- [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf?ref=https://githubhelp.com)

## X-Risk

- **[X-Risk Analysis for AI Research](https://drive.google.com/file/d/1kUroMFoGc7Zym3ce01EaRu2uysyRFJhx/view?usp=sharing)**

- **[Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg)**

- **[What happens when our computers get smarter than we are?](https://www.youtube.com/watch?v=MnT1xgZgkpk)**

---
layout: page
title: Readings
description: Course readings.
---

# Readings
{:.no_toc}

_Italicized_ resources are required, and other resources are suggested.

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

## Introduction

- _[Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)_

- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)

- [Workshop On Safety And Control For Artificial Intelligence](https://www.cmu.edu/safartint/watch.html)

### Deep Learning Review

- _[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)_

- _[Attention Is All You Need](https://arxiv.org/abs/1706.03762)_

- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

- [Layer Normalization](https://arxiv.org/abs/1607.06450)

- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)

- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)

- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

## Risk Analysis

- _[Power laws, Pareto distributions and Zipf’s law](https://arxiv.org/pdf/cond-mat/0412004.pdf)_

- _[What is a Complex System?](https://www.youtube.com/watch?v=vp8v2Udd_PM)_

- _[Emergence (Intermediate)](https://www.youtube.com/watch?v=QItTWZc7hKs)_

- _Introduction to STAMP ([video](https://www.youtube.com/watch?v=_ptmjAbacMk), [slides](http://psas.scripts.mit.edu/home/wp-content/uploads/2020/07/STAMP-Tutorial.pdf))_

- [Safe Design](https://risk-engineering.org/safe-design/), [Consequence assessment](https://risk-engineering.org/consequence-assessment/), [Safety models](https://risk-engineering.org/safety-models/)

- [A Brief History of Generative Models for Power Law and Lognormal Distributions](https://www.stat.berkeley.edu/~aldous/Networks/1089229510.pdf)

- [Empirical examples of power-law CDFs](https://youtu.be/KKYhPPf4FxA?t=345)

- [Log-normal distributions (with comparisons to power laws)](https://youtu.be/KiTAyRZORtQ?t=124)

- [Multiplicative processes produce log normals](https://www.youtube.com/watch?v=yA1pkyanbzw)

- [Multiplicative processes produce power laws](https://youtu.be/B43yhWxdi5I?t=69)

- [Review of properties of power laws](https://www.youtube.com/watch?v=9Hc227Qy91k)

- [Numerous power laws for cities](https://www.youtube.com/watch?v=DsL7jEQXh8I)

- [The Black Swan](https://www.youtube.com/watch?v=caPy0OZmXKs) and
  [Antifragile](https://www.youtube.com/watch?v=-MMLea-_ifw) Summaries

- [The Precautionary Principle (skip second half)](https://arxiv.org/abs/1410.5787)

- [Emergence (Basic)](https://www.youtube.com/watch?v=16W7c0mb-rE)

- [Nonlinear Causality](https://www.youtube.com/watch?v=76JRJ90s548)

- [Beyond Normal Accidents and High Reliability Organizations: The Need for an Alternative Approach to Safety in Complex Systems](http://sunnyday.mit.edu/papers/hro.pdf)

- [Shortcomings of the Bow Tie and Other Safety Tools Based on Linear Causality1](http://sunnyday.mit.edu/Bow-tie-final.pdf)

- [Systemantics Appendix](https://drive.google.com/file/d/1avoVTY8L3hpZi9fTxI_1mjjXC5JTz882/view?usp=sharing)

- [How Complex Systems Fail](https://how.complexsystems.fail/)

## Robustness

### Adversarial Robustness

- _[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)_

- _[Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)_

- [Universal Adversarial Triggers for Attacking and Analyzing NLP](https://arxiv.org/abs/1908.07125)

- [Data Augmentation Can Improve Robustness](https://arxiv.org/abs/2111.05328)

- [Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)

- [BERT-ATTACK: Adversarial Attack Against BERT Using BERT](https://arxiv.org/abs/2004.09984) ([GitHub](https://github.com/chbrian/awesome-adversarial-examples-dl))

- [Gradient-based Adversarial Attacks against Text
  Transformers](https://arxiv.org/abs/2104.13733)

- [Smooth Adversarial Training](https://arxiv.org/abs/2006.14536)

- [Reliable evaluation of adversarial robustness with an ensemble of diverse
  parameter-free attacks](https://arxiv.org/abs/2003.01690)
  ([website](https://robustbench.github.io/))

- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)

- [Adversarial Examples Are a Natural Consequence of Test Error in Noise](https://arxiv.org/abs/1901.10513)

- [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)

- [Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)

- [Certified Defenses against Adversarial Examples](https://arxiv.org/abs/1801.09344)

- [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)

### Long Tails and Distribution Shift

- _[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/abs/2006.16241)_

- _[Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/abs/1903.12261)_

- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)

- [WILDS: A Benchmark of in-the-Wild Distribution Shifts](https://arxiv.org/abs/2012.07421)

- [ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models](https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html)

- [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/abs/1910.14599)

- [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)

- [ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231)

## Monitoring

### OOD and Malicious Behavior Detection

- _[Deep Anomaly Detection with Outlier Exposure](https://arxiv.org/abs/1812.04606)_

- _[A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://arxiv.org/abs/1610.02136)_

- [ViM: Out-Of-Distribution with Virtual-logit Matching](https://arxiv.org/abs/2203.10807)

- [VOS: Learning What You Don't Know by Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)

- [Scaling Out-of-Distribution Detection for Real-World Settings](https://arxiv.org/abs/1911.11132)

- [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/abs/1807.03888)

### Interpretable Uncertainty

- _[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)_

- _[Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530)_

- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)

- [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474)

- [Posterior calibration and exploratory analysis for natural language processing models](https://arxiv.org/abs/1508.05154)

- [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)

### Transparency

- _[The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)_

- [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292)

- [Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/abs/1704.03296)

- [Locating and Editing Factual Knowledge in GPT](https://arxiv.org/abs/2202.05262)

- [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)

- [Feature Visualizations](https://distill.pub/2017/feature-visualization/) and [OpenAI Microscope](https://microscope.openai.com/)

- [Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization](https://arxiv.org/abs/2010.12606)

- [Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796)

- [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/abs/1811.10154)

- [Convergent Learning: Do different neural networks learn the same representations?](https://arxiv.org/abs/1511.07543)
 
### Trojans

- _[Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)_

- _[Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs](https://arxiv.org/abs/1906.10842)_

- _[Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)_

- [TrojAI](https://pages.nist.gov/trojai/)

- [Detecting AI Trojans Using Meta Neural Analysis](https://arxiv.org/abs/1910.03137)

- [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)

- [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)

- [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)

### Detecting and Forecasting Emergent Behavior

- _[The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://arxiv.org/abs/2201.03544)_

- _[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)_

- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

- [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683)

- [The Off-Switch Game](https://arxiv.org/abs/1611.08219)

- [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111)

## Alignment

### Power-Seeking

<!--<upcoming power aversion paper>-->

- [Existential Risk from Power-Seeking AI](https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf)
- [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark](https://arxiv.org/pdf/2304.03279.pdf)

### Honest AI

<!--<upcoming collin paper>-->

- [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)

- [Truthful AI: Developing and governing AI that does not lie](https://arxiv.org/abs/2110.06674)

### Machine Ethics

<!--<upcoming utility paper>-->

- _[What Would Jiminy Cricket Do? Towards Agents That Behave Morally](https://arxiv.org/abs/2110.13136)_

- _[Ethics Background (Introduction through "Absolute Rights or Prima Facie Duties")](https://www.youtube.com/playlist?list=PLKtXFotbf7fOg7zbQ3565EnpzzKlYaVVI)_

- [Aligning AI With Shared Human Values](https://arxiv.org/abs/2008.02275)

- [Avoiding Side Effects in Complex Environments](https://arxiv.org/abs/2006.06547)

- [Conservative Agency via Attainable Utility Preservation](https://arxiv.org/abs/1902.09725)

- [The Structure of Normative Ethics ](https://cpb-us-west-2-juc1ugur1qwqqqo4.stackpathdns.com/campuspress.yale.edu/dist/7/724/files/2016/01/The-Structure-of-Normative-Ethics-2gw2akt.pdf)

## Systemic Safety

### Forecasting
- [Forecasting Future World Events with Neural Networks](https://arxiv.org/abs/2206.15474)

- [On Single Point Forecasts for Fat-Tailed Variables](https://arxiv.org/abs/2007.16096)

- [On the Difference between Binary Prediction and True Exposure With Implications For Forecasting Tournaments and Decision Making Research](https://www.stat.berkeley.edu/~aldous/157/Papers/taleb_tetlock.pdf)

- [Superforecasting -- Philip Tetlock](https://youtu.be/pedNak4S9IE?t=440)

### ML for Cyberdefense

- [Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://arxiv.org/pdf/2108.09293.pdf?nylayout=pc)

### Cooperative AI

- [Uehiro Lectures 2022](https://www.practicalethics.ox.ac.uk/uehiro-lectures-2022)

- [Open Problems in Cooperative AI](https://arxiv.org/abs/2012.08630)

- [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf)

## X-Risk

- _[X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862)_

- [Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg)

- [What happens when our computers get smarter than we are?](https://www.youtube.com/watch?v=MnT1xgZgkpk)

- [X-Risk Motivations for Safety Research Directions](https://docs.google.com/document/d/1PXwjSbh-g1U1JEXhf55C7YsqD5qKNdBPQGgQ7W0Zm1A/edit?usp=sharing)

- [Natural Selection Favors AIs over Humans](https://arxiv.org/abs/2303.16200)
